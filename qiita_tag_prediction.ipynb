{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "qiita_tag_prediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m3yrin/topic-aware-tag-prediction/blob/master/qiita_tag_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsVNB0d-ivy2",
        "colab_type": "text"
      },
      "source": [
        "# Tag generation for Japanese article\n",
        "Re-implementation of \"Topic-Aware Neural Keyphrase Generation for Social Media Language\"\n",
        "\n",
        "Auther : @m3yrin\n",
        "\n",
        "## Reference\n",
        "* Papar\n",
        "Topic-Aware Neural Keyphrase Generation for Social Media Language  \n",
        "Yue Wang, Jing Li, Hou Pong Chan, Irwin King, Michael R. Lyu, Shuming Shi  \n",
        "https://arxiv.org/abs/1906.03889  \n",
        "ACL 2019 Long paper\n",
        "\n",
        "* https://github.com/yuewang-cuhk/TAKG\n",
        "* https://github.com/m3yrin/NTM\n",
        "\n",
        "* Qiita data gathering\n",
        "    * https://qiita.com/pocket_kyoto/items/64a5ae16f02023df883e\n",
        "    \"Qiitaの記事データは、機械学習のためのデータセットに向いている\"\n",
        "\n",
        "## Dataset\n",
        "Qiita articles. These are mainly technical articles written in Japanese.  \n",
        "https://qiita.com/\n",
        "\n",
        "You can gather articles through Qiita API.  \n",
        "https://qiita.com/api/v2/docs?locale=en\n",
        "\n",
        "## Memo\n",
        "### Some methods are not implemented\n",
        "Beam search and copy mechanism are not implemented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe-PIppEkyTF",
        "colab_type": "text"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTIrmcKCkrhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/m3yrin/topic-aware-tag-prediction.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WMGQQU28ryp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install janome"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZdAPOQw47bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('topic-aware-tag-prediction')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhmPESrHnhmJ",
        "colab_type": "text"
      },
      "source": [
        "### Data Gathering\n",
        "In this repository, dataset is not included.  \n",
        "About Qiita API, please see https://qiita.com/api/v2/docs?locale=en\n",
        "\n",
        "Sample script `qiita_api.py` is added to this repo. If you use `qiita_api.py`, please check its code before excution. Access token is required. https://qiita.com/settings/applications?locale=en    \n",
        "Usage :  \n",
        "```bash\n",
        "python qiita-api.py -auth_token <your_qiita_api_access_token> -data_dir ./ -start_date 2019-01-01 -end_date 2019-02-01\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BsjP6islm1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python qiita-api.py -auth_token <your_qiita_api_auth_token> -data_dir ./ -start_date 2019-01-01 -end_date 2019-02-01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psHop3ojp7uu",
        "colab_type": "text"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC_QI-oT4KLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "\n",
        "import janome\n",
        "from janome import analyzer\n",
        "from janome.charfilter import *\n",
        "from janome.tokenfilter import *\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device :',device)\n",
        "\n",
        "# set random seeds\n",
        "random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "random_state = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1PM38Vci259",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU0EdbIz4KMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOME_DIR = \"./\"\n",
        "bow = pd.read_pickle(HOME_DIR+\"bow.pkl\")\n",
        "text = pd.read_pickle(HOME_DIR+\"text.pkl\")\n",
        "target = pd.read_pickle(HOME_DIR+\"target.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfaOjJz8OwFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# discard words over 400 for fast training.\n",
        "text_short = [s[:400] for s in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FikABIz-OuYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, test_X, train_B, test_B, train_Y, test_Y \\\n",
        "    = train_test_split(text_short, bow, target, test_size=0.2, random_state=random_state)\n",
        "\n",
        "train_X, valid_X, train_B, valid_B, train_Y, valid_Y \\\n",
        "    = train_test_split(train_X, train_B, train_Y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "print(\"# of train, valid, test :\", len(train_X),len(valid_X), len(test_X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W97qeYv5itZG",
        "colab_type": "text"
      },
      "source": [
        "### Making vocab for sequence data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFNxysDTR4HK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_ids(vocab, sentence):\n",
        "    ids = [vocab.word2id.get(word, UNK) for word in sentence]\n",
        "    ids += [EOS]\n",
        "    return ids\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, word2id={}):\n",
        "        \n",
        "        self.word2id = dict(word2id)\n",
        "        self.id2word = {v: k for k, v in self.word2id.items()}    \n",
        "        \n",
        "    def build_vocab(self, sentences, min_count=1):\n",
        "        word_counter = {}\n",
        "        for sentence in sentences:\n",
        "            for word in sentence:\n",
        "                word_counter[word] = word_counter.get(word, 0) + 1\n",
        "\n",
        "        for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
        "            if count < min_count:\n",
        "                break\n",
        "            _id = len(self.word2id)\n",
        "            self.word2id.setdefault(word, _id)\n",
        "            self.id2word[_id] = word "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrH3lcjBjBDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# special tokens\n",
        "PAD_TOKEN = '<PAD>'\n",
        "BOS_TOKEN = '<S>'\n",
        "EOS_TOKEN = '</S>'\n",
        "UNK_TOKEN = '<UNK>'\n",
        "PAD = 0\n",
        "BOS = 1\n",
        "EOS = 2\n",
        "UNK = 3\n",
        "\n",
        "word2id = {\n",
        "    PAD_TOKEN: PAD,\n",
        "    BOS_TOKEN: BOS,\n",
        "    EOS_TOKEN: EOS,\n",
        "    UNK_TOKEN: UNK,\n",
        "    }\n",
        "\n",
        "# minimun acceptable count for input sequence\n",
        "MIN_COUNT = 3\n",
        "\n",
        "# build vocab\n",
        "vocab_X = Vocab(word2id=word2id)\n",
        "vocab_Y = Vocab(word2id=word2id)\n",
        "vocab_X.build_vocab(train_X, min_count=MIN_COUNT)\n",
        "vocab_Y.build_vocab(train_Y, min_count=1)\n",
        "\n",
        "vocab_size_X = len(vocab_X.id2word)\n",
        "vocab_size_Y = len(vocab_Y.id2word)\n",
        "print('# of input vocab ：', vocab_size_X)\n",
        "print('# of output vocab：', vocab_size_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AoRotlejEgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize\n",
        "train_X = [sentence_to_ids(vocab_X, sentence) for sentence in train_X]\n",
        "train_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in train_Y]\n",
        "valid_X = [sentence_to_ids(vocab_X, sentence) for sentence in valid_X]\n",
        "valid_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in valid_Y]\n",
        "test_X = [sentence_to_ids(vocab_X, sentence) for sentence in test_X]\n",
        "test_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in test_Y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-cSDAt5kCu-",
        "colab_type": "text"
      },
      "source": [
        "### Making vocab for BoW data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubjEEH4UipoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_bow_vocab(data, no_below=5, no_above=0.2):\n",
        "\n",
        "    bow_dictionary = gensim.corpora.Dictionary(data)\n",
        "    bow_dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
        "    \n",
        "    # Re-id\n",
        "    bow_dictionary.compactify()\n",
        "    bow_dictionary.id2token = dict([(id, t) for t, id in bow_dictionary.token2id.items()])\n",
        "    \n",
        "    print(\"BOW dict length : %d\" % len(bow_dictionary))\n",
        "    \n",
        "    return bow_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USEWVeyV6zmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_vocab = build_bow_vocab(train_B)\n",
        "bow_vocab_size=len(bow_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIfLr30AiSg9",
        "colab_type": "text"
      },
      "source": [
        "### Dataloader definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b22QJq0LS75C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader(object):\n",
        "\n",
        "    def __init__(self, X, B, Y, bow_vocab, batch_size, shuffle=True):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.bow_vocab = bow_vocab\n",
        "        \n",
        "        self.index = 0\n",
        "        self.pointer = np.array(range(len(X)))\n",
        "        \n",
        "        BV = [bow_vocab.doc2bow(s) for s in B]\n",
        "        self.data = np.array(list(zip(X, B, BV, Y)))\n",
        "        \n",
        "        # counting total word number\n",
        "        word_count = []\n",
        "        for bow in BV:\n",
        "            wc = 0\n",
        "            for (i, c) in bow:\n",
        "                wc += c\n",
        "            word_count.append(wc)\n",
        "        \n",
        "        self.word_count = sum(word_count)\n",
        "        self.data_size = len(X)\n",
        "        \n",
        "        self.shuffle = shuffle\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self.shuffle:\n",
        "            self.pointer = shuffle(self.pointer)\n",
        "        self.index = 0 \n",
        "    \n",
        "    # transform bow data into (1 x V) size vector.\n",
        "    def _pad(self, batch):\n",
        "        bow_vocab = len(self.bow_vocab)\n",
        "        res_src_bow = np.zeros((len(batch), bow_vocab))\n",
        "        \n",
        "        for idx, bow in enumerate(batch):\n",
        "            bow_k = [k for k, v in bow]\n",
        "            bow_v = [v for k, v in bow]\n",
        "            res_src_bow[idx, bow_k] = bow_v\n",
        "            \n",
        "        return res_src_bow\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \n",
        "        if self.index >= self.data_size:\n",
        "            self.reset()\n",
        "            raise StopIteration()\n",
        "            \n",
        "        ids = self.pointer[self.index: self.index + self.batch_size]\n",
        "        seqs_X, seqs_B, seqs_BV, seqs_Y = zip(*self.data[ids])\n",
        "        \n",
        "        # sort for rnn\n",
        "        seq_pairs = sorted(zip(seqs_X, seqs_B, seqs_BV, seqs_Y), key=lambda p: len(p[0]), reverse=True)\n",
        "        seqs_X, seqs_B, seqs_BV, seqs_Y = zip(*seq_pairs)\n",
        "        \n",
        "        lengths_X = [len(s) for s in seqs_X]\n",
        "        lengths_Y = [len(s) for s in seqs_Y]\n",
        "        max_length_X = max(lengths_X)\n",
        "        max_length_Y = max(lengths_Y)\n",
        "        padded_X = [pad_seq(s, max_length_X) for s in seqs_X]\n",
        "        padded_Y = [pad_seq(s, max_length_Y) for s in seqs_Y]\n",
        "        \n",
        "        padded_BV = self._pad(seqs_BV)\n",
        "        \n",
        "        # transposed for rnn\n",
        "        batch_X = torch.tensor(padded_X, dtype=torch.long, device=device).transpose(0, 1)\n",
        "        batch_Y = torch.tensor(padded_Y, dtype=torch.long, device=device).transpose(0, 1)\n",
        "        \n",
        "        batch_B = seqs_B\n",
        "        batch_BV = torch.tensor(padded_BV, dtype=torch.float, device=device)\n",
        "        \n",
        "        self.index += self.batch_size\n",
        "        \n",
        "        return batch_X, batch_B, batch_BV, batch_Y, lengths_X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz5HKAzahlGV",
        "colab_type": "text"
      },
      "source": [
        "### Model definition of NTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZGlUjQ6hmhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cited : https://github.com/yuewang-cuhk/TAKG/blob/master/pykp/model.py\n",
        "\n",
        "class NTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, topic_num,  l1_strength=0.001):\n",
        "        super(NTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.topic_num = topic_num\n",
        "        self.fc11 = nn.Linear(self.input_dim, hidden_dim)\n",
        "        self.fc12 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc21 = nn.Linear(hidden_dim, topic_num)\n",
        "        self.fc22 = nn.Linear(hidden_dim, topic_num)\n",
        "        self.fcs = nn.Linear(self.input_dim, hidden_dim, bias=False)\n",
        "        self.fcg1 = nn.Linear(topic_num, topic_num)\n",
        "        self.fcg2 = nn.Linear(topic_num, topic_num)\n",
        "        self.fcg3 = nn.Linear(topic_num, topic_num)\n",
        "        self.fcg4 = nn.Linear(topic_num, topic_num)\n",
        "\n",
        "        # bias disabled\n",
        "        self.fcd1 = nn.Linear(topic_num, self.input_dim, bias=False)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        e1 = F.relu(self.fc11(x))\n",
        "        e1 = F.relu(self.fc12(e1))\n",
        "        e1 = e1.add(self.fcs(x))\n",
        "        return self.fc21(e1), self.fc22(e1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def generate(self, h):\n",
        "        g1 = torch.tanh(self.fcg1(h))\n",
        "        g1 = torch.tanh(self.fcg2(g1))\n",
        "        g1 = torch.tanh(self.fcg3(g1))\n",
        "        g1 = torch.tanh(self.fcg4(g1))\n",
        "        g1 = g1.add(h)\n",
        "        return g1\n",
        "\n",
        "    def decode(self, z):\n",
        "        d1 = F.softmax(self.fcd1(z), dim=1)\n",
        "        return d1\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        g = self.generate(z)\n",
        "        return z, g, self.decode(g), mu, logvar\n",
        "\n",
        "    def print_topic_words(self, vocab_dic, fn, n_top_words=10):\n",
        "        beta_exp = self.fcd1.weight.data.cpu().numpy().T\n",
        "        \n",
        "        for k, beta_k in enumerate(beta_exp):\n",
        "            topic_words = [vocab_dic[w_id] for w_id in np.argsort(beta_k)[:-n_top_words - 1:-1]]\n",
        "            print('Topic {}: {}'.format(k, ' '.join(topic_words)))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms9jb_5Dkoz9",
        "colab_type": "text"
      },
      "source": [
        "### Model Definition of Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz08MUgCS77j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=PAD)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, seqs, input_lengths, hidden=None):\n",
        "        emb = self.embedding(seqs)\n",
        "        packed = pack_padded_sequence(emb, input_lengths)\n",
        "        output, hidden = self.gru(packed, hidden)\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5vbldQUw6uD",
        "colab_type": "text"
      },
      "source": [
        "#### Memo\n",
        "https://arxiv.org/abs/1906.03889  \n",
        "Topic-Aware Neural Keyphrase Generation for Social Media Language\n",
        "\n",
        "\n",
        "* decoder RNN\n",
        "$$\n",
        "\\mathbf{s}_{j}=f_{G R U}\\left(\\left[\\mathbf{u}_{j} ; \\theta\\right], \\mathbf{s}_{j-1}\\right)\n",
        "$$\n",
        "\n",
        "* Attention\n",
        "$$\n",
        "\\alpha_{i j}=\\frac{\\exp \\left(f_{\\alpha}\\left(\\mathbf{h}_{i}, \\mathbf{s}_{j}, \\theta\\right)\\right)}{\\sum_{i^{\\prime}=1}^{|\\mathbf{x}|} \\exp \\left(f_{\\alpha}\\left(\\mathbf{h}_{i^{\\prime}}, \\mathbf{s}_{j}, \\theta\\right)\\right)}, \\ \\mathbf{c}_{j}=\\sum_{i=1}^{|\\mathbf{x}|} \\alpha_{i j} \\mathbf{h}_{i}\n",
        "$$\n",
        "\n",
        "* Attention function\n",
        "\n",
        "$$\n",
        "f_{\\alpha}(h_i, s_j , \\theta) = v_{\\alpha}^T tanh(W_{\\alpha}[h_i; s_j ; \\theta] + b_{\\alpha}).\n",
        "$$\n",
        "\n",
        "* $p_{gen}$\n",
        "$$\n",
        "p_{g e n}={softmax}\\left(\\mathbf{W}_{g e n}\\left[\\mathbf{s}_{j} ; \\mathbf{c}_{j}\\right]+\\mathbf{b}_{g e n}\\right)\n",
        "$$\n",
        "\n",
        "* Copy mechanism (Not implemented.)\n",
        "$$\\lambda_{j}={sigmoid}\\left(\\mathbf{W}_{\\lambda}\\left[\\mathbf{u}_{j} ; \\mathbf{s}_{j} ; \\mathbf{c}_{j} ; \\theta\\right]+\\mathbf{b}_{\\lambda}\\right)$$\n",
        "$$ \n",
        "p_{j}=\\lambda_{j} \\cdot p_{g e n}+\\left(1-\\lambda_{j}\\right) \\cdot \\sum_{i=1}^{|\\mathbf{x}|} \\alpha_{i j}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j__DacSzS7-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD)\n",
        "        self.gru = nn.GRU(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
        "        \n",
        "        self.W_a  = torch.rand((hidden_size * 3, hidden_size), dtype=torch.float,device=device , requires_grad=True)\n",
        "        self.b    = torch.rand(hidden_size, dtype=torch.float, device=device, requires_grad=True)\n",
        "        self.v  = torch.rand((hidden_size, 1), dtype=torch.float,device=device , requires_grad=True)\n",
        "        #self.f_copy = nn.Linear(hidden_size * 4, 1)\n",
        "    \n",
        "    def forward(self, seqs, hidden, encoder_output, latent_topic):\n",
        "        emb = self.embedding(seqs)\n",
        "        emb = torch.cat((emb, latent_topic), 2)\n",
        "        \n",
        "        _, hidden = self.gru(emb, hidden)\n",
        "\n",
        "        attn, a_weight = self.attention(hidden, encoder_output, latent_topic)\n",
        "        \n",
        "        p_gen = torch.cat((hidden, attn), 2)\n",
        "        p_gen = self.out(p_gen)\n",
        "\n",
        "        # Copy mechanism\n",
        "        #lamda_pgen = torch.sigmoid(self.f_copy(torch.cat((emb, hidden, attn), 2)))\n",
        "        #output = lamda_pgen * p_gen \n",
        "        #output += (1.0 - lamda_pgen) * attn\n",
        "\n",
        "        output = p_gen\n",
        "        \n",
        "        return output, hidden\n",
        "    \n",
        "    def attention(self, u, encoder_output, latent_topic):\n",
        "        \n",
        "        \"\"\"\n",
        "        u              : embeded decoder input, (1, batch, hidden_size)\n",
        "        encoder_output : encoder outputs,       (seq_len, batch, hidden_size)\n",
        "        latent_topic   : topic vector from ntm, (1, batch, hidden_size)\n",
        "        \"\"\"\n",
        "        \n",
        "        seq_len = len(encoder_output)\n",
        "        \n",
        "        # -> (batch, seq_len, hidden_size)\n",
        "        e_output = encoder_output.transpose(0,1)\n",
        "        \n",
        "        # state_topic : (1, batch, hidden_size * 2)\n",
        "        state_topic = torch.cat((u, latent_topic), 2)\n",
        "        \n",
        "        # -> state_topic : (seq_len, batch, hidden_size * 2)\n",
        "        state_topic = [state_topic] * seq_len\n",
        "        state_topic = torch.cat(state_topic, 0)\n",
        "        \n",
        "        # -> state_topic : (batch, seq_len, hidden_size * 2)\n",
        "        state_topic = state_topic.transpose(0,1)\n",
        "        \n",
        "        # state_topic_output : (batch, seq_len, hidden_size * 3)\n",
        "        state_topic_output = torch.cat((state_topic, e_output), 2)\n",
        "        \n",
        "        # state_topic_output: (batch(i), seq_len(j), hidden_size * 3 (k))\n",
        "        # self.W_a  : [hidden_size * 3(k), hidden_size(l)]\n",
        "        # -> atten_weight : (batch(i), seq_len(j), hidden_size(l))\n",
        "        \n",
        "        atten_weight = torch.einsum('ijk,kl->ijl', state_topic_output, self.W_a)\n",
        "        atten_weight = torch.tanh(atten_weight + self.b)\n",
        "        \n",
        "        # atten_weight : (batch(i), seq_len(j), hidden_size(k))\n",
        "        # self.v : (hidden_size(k), 1(l))\n",
        "        # -> atten_weight : (batch(i), seq_len(j), 1)\n",
        "        atten_weight = torch.matmul(atten_weight, self.v)\n",
        "        atten_weight = F.softmax(atten_weight, dim=1)\n",
        "        \n",
        "        # atten_weight : (batch(i), seq_len(j), 1(k))\n",
        "        # e_output     : (batch(i), seq_len(j), hidden_size(l))\n",
        "        # -> c : (batch(i), hidden_size(l))\n",
        "        c = torch.einsum('ijk,ijl->il', atten_weight, e_output)\n",
        "        \n",
        "        # -> (1, batch, hidden_size)\n",
        "        return c.unsqueeze(0), atten_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPKl2IBlkukB",
        "colab_type": "text"
      },
      "source": [
        "### Gathering models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2DuIv1aTEUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size_ntm, output_topic_num, hidden_size_ntm, input_size_s2s, output_size_s2s, hidden_size_s2s, l1_strength=0.001):\n",
        "        super(Model, self).__init__()\n",
        "        self.ntm = NTM(input_size_ntm, hidden_size_ntm, hidden_size_s2s, l1_strength=l1_strength)\n",
        "        self.encoder = Encoder(input_size_s2s, hidden_size_s2s)\n",
        "        self.decoder = Decoder(hidden_size_s2s, output_size_s2s)\n",
        "        \n",
        "    def forward(self, batch_X, batch_BV, lengths_X, max_length, mode, batch_Y=None, use_teacher_forcing=False):\n",
        "        \n",
        "        if mode == 'ntm':\n",
        "            #ntm\n",
        "            z, g, recon_batch, mu, logvar = self.ntm(batch_BV)\n",
        "            \n",
        "            return None, z, g, recon_batch, mu, logvar\n",
        "            \n",
        "        elif mode == 's2s':\n",
        "            #ntm\n",
        "            z, g, recon_batch, mu, logvar = self.ntm(batch_BV)\n",
        "            \n",
        "            # use g as laten topic vector, profibit back prop to ntm module\n",
        "            latent_topic = g.detach().unsqueeze(0)\n",
        "            \n",
        "            # s2s\n",
        "            encoder_output, encoder_hidden = self.encoder(batch_X, lengths_X)\n",
        "            \n",
        "            _batch_size = batch_X.size(1)\n",
        "            decoder_input = torch.tensor([BOS] * _batch_size, dtype=torch.long, device=device)\n",
        "            decoder_input = decoder_input.unsqueeze(0)\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            decoder_outputs = torch.zeros(max_length, _batch_size, self.decoder.output_size, device=device)\n",
        "\n",
        "            for t in range(max_length):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output,  latent_topic)\n",
        "                \n",
        "                decoder_outputs[t] = decoder_output\n",
        "                if use_teacher_forcing and batch_Y is not None:\n",
        "                    decoder_input = batch_Y[t].unsqueeze(0)\n",
        "                else:\n",
        "                    decoder_input = decoder_output.max(-1)[1]\n",
        "                \n",
        "            return decoder_outputs, z, g, recon_batch, mu, logvar\n",
        "        \n",
        "        else:\n",
        "            #ntm\n",
        "            z, g, recon_batch, mu, logvar = self.ntm(batch_BV)\n",
        "            \n",
        "            # use g as laten topic vector\n",
        "            latent_topic = g.unsqueeze(0)\n",
        "            \n",
        "            # s2s\n",
        "            encoder_output, encoder_hidden = self.encoder(batch_X, lengths_X)\n",
        "            \n",
        "            _batch_size = batch_X.size(1)\n",
        "            decoder_input = torch.tensor([BOS] * _batch_size, dtype=torch.long, device=device)\n",
        "            decoder_input = decoder_input.unsqueeze(0)\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            decoder_outputs = torch.zeros(max_length, _batch_size, self.decoder.output_size, device=device)\n",
        "\n",
        "            for t in range(max_length):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output, latent_topic)\n",
        "                \n",
        "                decoder_outputs[t] = decoder_output\n",
        "                if use_teacher_forcing and batch_Y is not None:\n",
        "                    decoder_input = batch_Y[t].unsqueeze(0)\n",
        "                else:\n",
        "                    decoder_input = decoder_output.max(-1)[1]\n",
        "                \n",
        "            return decoder_outputs, z, g, recon_batch, mu, logvar\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDkag4f8hvse",
        "colab_type": "text"
      },
      "source": [
        "### AUX functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnbMnprX1P2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.kaiming_uniform(m.weight)\n",
        "\n",
        "def pad_seq(seq, max_length):\n",
        "    res = seq + [PAD for i in range(max_length - len(seq))]\n",
        "    return res    \n",
        "\n",
        "mce = nn.CrossEntropyLoss(size_average=False, ignore_index=PAD)\n",
        "def masked_cross_entropy(logits, target):\n",
        "    return mce(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "\n",
        "def l1_penalty(para):\n",
        "    return nn.L1Loss()(para, torch.zeros_like(para))\n",
        "\n",
        "def check_sparsity(para, sparsity_threshold=1e-3):\n",
        "    num_weights = para.shape[0] * para.shape[1]\n",
        "    num_zero = (para.abs() < sparsity_threshold).sum().float()\n",
        "    return num_zero / float(num_weights)\n",
        "\n",
        "def update_sparsity_l1(model, sparsity_target):\n",
        "    \n",
        "    cur_sparsity = check_sparsity(model.ntm.fcd1.weight.data)\n",
        "    cur_l1 = model.ntm.l1_strength\n",
        "    \n",
        "    diff = sparsity_target - cur_sparsity\n",
        "    cur_l1.mul_(2.0 ** diff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OktlvWgU43wk",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiESgagCUW_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QpJfh5lrG53",
        "colab_type": "text"
      },
      "source": [
        "### 1st Training (NTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr-ueh-I1bEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss_ntm(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer=None, is_train=True, l1_strength = 1e5):\n",
        "    model.train(is_train)\n",
        "    \n",
        "    # dummy\n",
        "    use_teacher_forcing = is_train and (random.random() < teacher_forcing_rate)\n",
        "    max_length = batch_Y.size(0)\n",
        "\n",
        "    # norm bow vector\n",
        "    batch_BV_norm = F.normalize(batch_BV)\n",
        "    \n",
        "    # forward all model\n",
        "    _, z, g, recon_batch, mu, logvar = model(batch_X, batch_BV_norm, lengths_X, max_length, 'ntm', batch_Y, use_teacher_forcing)\n",
        "    \n",
        "    # loss for ntm\n",
        "    bce = F.binary_cross_entropy(recon_batch, batch_BV, size_average=False)\n",
        "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    loss_ntm = bce + kld\n",
        "    \n",
        "    # add l1 penalty for sparsity of ntm decoder FC weight\n",
        "    loss_ntm += l1_strength * l1_penalty(model.ntm.fcd1.weight)\n",
        "\n",
        "    # sum up losses\n",
        "    loss = loss_ntm\n",
        "    \n",
        "    if is_train:\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    return loss.item(), bce.item(), _"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwqXbWv-Ugp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameter\n",
        "batch_size = 128\n",
        "num_epochs_ntm = 100\n",
        "lr = 1e-3\n",
        "target_sparsity=0.85\n",
        "teacher_forcing_rate = 0.0\n",
        "l1_strength = 1e6\n",
        "\n",
        "# Model parameter\n",
        "model_args = {\n",
        "    'input_size_ntm'  : bow_vocab_size,\n",
        "    'output_topic_num': 256,\n",
        "    'hidden_size_ntm' : 256,\n",
        "    'input_size_s2s'  : vocab_size_X, \n",
        "    'output_size_s2s' : vocab_size_Y,\n",
        "    'hidden_size_s2s' : 256\n",
        "}\n",
        "\n",
        "train_dataloader = DataLoader(train_X, train_B, train_Y, bow_vocab, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_X, valid_B, valid_Y, bow_vocab, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = Model(**model_args).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOy8jQN3ycqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorboard\n",
        "writer = SummaryWriter()\n",
        "\n",
        "for epoch in range(1, num_epochs_ntm+1):\n",
        "    train_loss = 0.\n",
        "    train_bce  = 0.\n",
        "    valid_loss = 0.\n",
        "    valid_bce  = 0.\n",
        "    \n",
        "    # train loop\n",
        "    for batch in train_dataloader:\n",
        "        batch_X, _, batch_BV, batch_Y, lengths_X = batch\n",
        "        loss, bce, _ = compute_loss_ntm(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer, is_train=True, l1_strength = l1_strength)\n",
        "        train_loss += loss\n",
        "        train_bce  += bce\n",
        "        \n",
        "    # validation loop\n",
        "    for batch in valid_dataloader:\n",
        "        batch_X, _, batch_BV, batch_Y, lengths_X = batch\n",
        "        loss, bce, _ = compute_loss_ntm(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer, is_train=False, l1_strength = l1_strength)\n",
        "        valid_loss += loss\n",
        "        valid_bce  += bce\n",
        "        \n",
        "    # calc ave total loss\n",
        "    train_loss = np.sum(train_loss) / len(train_dataloader.data)\n",
        "    valid_loss = np.sum(valid_loss) / len(valid_dataloader.data)\n",
        "    \n",
        "    # calc ntm weight sparsity\n",
        "    sparsity = check_sparsity(model.ntm.fcd1.weight.data)\n",
        "    \n",
        "    # calc perplexity\n",
        "    train_bce = train_bce / train_dataloader.word_count\n",
        "    valid_bce = valid_bce / valid_dataloader.word_count\n",
        "    train_ppl = np.exp(train_bce)\n",
        "    valid_ppl = np.exp(valid_bce)\n",
        "    \n",
        "    print('Epoch {:4d} | loss(train/valid) {:5.2f} / {:5.2f}, ppl(train/valid) {:5.2f} / {:5.2f}, sparsity(current/target) {:.3f} / {:.3f},'.format(\n",
        "        epoch, train_loss, valid_loss,train_ppl, valid_ppl,float(sparsity.cpu()), target_sparsity))\n",
        "    \n",
        "    # tensorboard\n",
        "    writer.add_scalars('ntm/loss',{'train_loss': train_loss,'valid_loss': valid_loss},epoch)\n",
        "    writer.add_scalars('ntm/perplexity',{'train_ppl': train_ppl,'valid_ppl': valid_ppl},epoch)\n",
        "    writer.add_scalars('ntm/sparsity',{'sparsity': float(sparsity.cpu())},epoch)\n",
        "    \n",
        "    print('-'*80)\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r2DeH3DPW4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt = model.state_dict()\n",
        "torch.save(ckpt, HOME_DIR + 'model_ntm.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV-7IuMXKkG6",
        "colab_type": "text"
      },
      "source": [
        "### 2nd Training (Seq2seq)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3nz6x2qT_vW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameter\n",
        "batch_size = 128\n",
        "num_epochs_s2s = 10\n",
        "lr = 1e-3\n",
        "target_sparsity=0.85\n",
        "teacher_forcing_rate = 0.4\n",
        "l1_strength = 1e6\n",
        "\n",
        "# Model parameter\n",
        "model_args = {\n",
        "    'input_size_ntm'  : bow_vocab_size,\n",
        "    'output_topic_num': 256,\n",
        "    'hidden_size_ntm' : 256,\n",
        "    'input_size_s2s'  : vocab_size_X, \n",
        "    'output_size_s2s' : vocab_size_Y,\n",
        "    'hidden_size_s2s' : 256\n",
        "}\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_X, train_B, train_Y, bow_vocab, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_X, valid_B, valid_Y, bow_vocab, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = Model(**model_args).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "ckpt = torch.load(HOME_DIR + 'model_ntm.pt')\n",
        "model.load_state_dict(ckpt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETM2-L7RSfS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss_s2s(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer=None, is_train=True, gamma = 1.0, l1_strength = 1e6):\n",
        "    model.train(is_train)\n",
        "\n",
        "    # set params for s2s\n",
        "    use_teacher_forcing = is_train and (random.random() < teacher_forcing_rate)\n",
        "    max_length = batch_Y.size(0)\n",
        "    \n",
        "    # norm bow vector\n",
        "    batch_BV_norm = F.normalize(batch_BV)\n",
        "    \n",
        "    # forward all model\n",
        "    pred_Y, z, g, recon_batch, mu, logvar = model(batch_X, batch_BV_norm, lengths_X, max_length, 's2s', batch_Y, use_teacher_forcing)\n",
        "    \n",
        "    # loss for s2s\n",
        "    loss_s2s = masked_cross_entropy(pred_Y.contiguous(), batch_Y.contiguous())\n",
        "    \n",
        "    # loss for ntm\n",
        "    bce = F.binary_cross_entropy(recon_batch, batch_BV, size_average=False)\n",
        "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    #loss_ntm = bce + kld + model.ntm.l1_strength * l1_penalty(model.ntm.fcd1.weight)\n",
        "    loss_ntm = bce + kld\n",
        "\n",
        "    # sum up losses. gamma is a weight for loss_s2s\n",
        "    loss = loss_s2s * gamma\n",
        "    \n",
        "    if is_train:\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().T.tolist()\n",
        "\n",
        "    return loss.item(), bce.item(), pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxsuB3NHrM-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorboard\n",
        "writer = SummaryWriter()\n",
        "\n",
        "for epoch in range(1, num_epochs_s2s+1):\n",
        "    train_loss = 0.\n",
        "    valid_loss = 0.\n",
        "    \n",
        "    # train loop\n",
        "    for batch in train_dataloader:\n",
        "        batch_X, _, batch_BV, batch_Y, lengths_X = batch\n",
        "        loss, _, pred = compute_loss_s2s(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer, is_train=True, gamma=200)\n",
        "        train_loss += loss\n",
        "    \n",
        "    # validation loop\n",
        "    for batch in valid_dataloader:\n",
        "        batch_X, _, batch_BV, batch_Y, lengths_X = batch\n",
        "        loss, _, pred = compute_loss_s2s(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer, is_train=False, gamma=200)\n",
        "        valid_loss += loss\n",
        "        \n",
        "    # calc ave total loss\n",
        "    train_loss = np.sum(train_loss) / len(train_dataloader.data)\n",
        "    valid_loss = np.sum(valid_loss) / len(valid_dataloader.data)\n",
        "    \n",
        "    print('Epoch {:4d} | loss(train/valid) {:5.2f} / {:5.2f}'.format(epoch, train_loss, valid_loss))\n",
        "    \n",
        "    # tensorboard\n",
        "    writer.add_scalars('s2s/loss',{'train_loss': train_loss,'valid_loss': valid_loss},epoch)\n",
        "\n",
        "    print('-'*80)\n",
        "    \n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urtq7LeEcNPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt = model.state_dict()\n",
        "torch.save(ckpt, HOME_DIR + 'model_ntm_s2s.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "velv67sXrMef",
        "colab_type": "text"
      },
      "source": [
        "### 3rd Training (Joint)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au6FXaIwqwmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer=None, is_train=True, gamma = 1.0, l1_strength = 1e4):\n",
        "    model.train(is_train)\n",
        "\n",
        "    # set params for s2s\n",
        "    use_teacher_forcing = is_train and (random.random() < teacher_forcing_rate)\n",
        "    max_length = batch_Y.size(0)\n",
        "    \n",
        "    # norm bow vector\n",
        "    batch_BV_norm = F.normalize(batch_BV)\n",
        "    \n",
        "    # forward all model\n",
        "    pred_Y, z, g, recon_batch, mu, logvar = model(batch_X, batch_BV_norm, lengths_X, max_length, '', batch_Y, use_teacher_forcing)\n",
        "    \n",
        "    # loss for s2s\n",
        "    loss_s2s = masked_cross_entropy(pred_Y.contiguous(), batch_Y.contiguous())\n",
        "    \n",
        "    # loss for ntm\n",
        "    bce = F.binary_cross_entropy(recon_batch, batch_BV, size_average=False)\n",
        "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    #loss_ntm = bce + kld + model.ntm.l1_strength * l1_penalty(model.ntm.fcd1.weight)\n",
        "    loss_ntm = bce + kld + l1_strength * l1_penalty(model.ntm.fcd1.weight)\n",
        "\n",
        "    # sum up losses\n",
        "    loss = loss_ntm + loss_s2s * gamma\n",
        "    \n",
        "    if is_train:\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().T.tolist()\n",
        "\n",
        "    return loss.item(), bce.item(), pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viBy7dkTZycU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameter\n",
        "batch_size = 256\n",
        "num_epochs_all = 20\n",
        "lr = 1e-5\n",
        "target_sparsity=0.85\n",
        "teacher_forcing_rate = 0.4\n",
        "l1_strength = 1e6\n",
        "\n",
        "# Model parameter\n",
        "model_args = {\n",
        "    'input_size_ntm'  : bow_vocab_size,\n",
        "    'output_topic_num': 256,\n",
        "    'hidden_size_ntm' : 256,\n",
        "    'input_size_s2s'  : vocab_size_X, \n",
        "    'output_size_s2s' : vocab_size_Y,\n",
        "    'hidden_size_s2s' : 256\n",
        "}\n",
        "\n",
        "train_dataloader = DataLoader(train_X, train_B, train_Y, bow_vocab, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_X, valid_B, valid_Y, bow_vocab, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = Model(**model_args).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "ckpt = torch.load(HOME_DIR + 'model_ntm_s2s.pt')\n",
        "model.load_state_dict(ckpt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKVwwAO8TEkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorboard\n",
        "writer = SummaryWriter()\n",
        "\n",
        "for epoch in range(1, num_epochs_all+1):\n",
        "    train_loss = 0.\n",
        "    train_bce  = 0.\n",
        "    valid_loss = 0.\n",
        "    valid_bce  = 0.\n",
        "    \n",
        "    # train loop\n",
        "    for batch in train_dataloader:\n",
        "        batch_X, _, batch_BV, batch_Y, lengths_X = batch\n",
        "        loss, bce, pred = compute_loss(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer, is_train=True, gamma=200, l1_strength = l1_strength)\n",
        "        train_loss += loss\n",
        "        train_bce  += bce\n",
        "        \n",
        "    # validation loop\n",
        "    for batch in valid_dataloader:\n",
        "        batch_X, _, batch_BV, batch_Y, lengths_X = batch\n",
        "        loss, bce, pred = compute_loss(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer, is_train=False, gamma=200, l1_strength = l1_strength)\n",
        "        valid_loss += loss\n",
        "        valid_bce  += bce\n",
        "        \n",
        "    # calc ave total loss\n",
        "    train_loss = np.sum(train_loss) / len(train_dataloader.data)\n",
        "    valid_loss = np.sum(valid_loss) / len(valid_dataloader.data)\n",
        "    \n",
        "    # calc ntm weight sparsity\n",
        "    sparsity = check_sparsity(model.ntm.fcd1.weight.data)\n",
        "    \n",
        "    # calc perplexity\n",
        "    train_bce = train_bce / train_dataloader.word_count\n",
        "    valid_bce = valid_bce / valid_dataloader.word_count\n",
        "    train_ppl = np.exp(train_bce)\n",
        "    valid_ppl = np.exp(valid_bce)\n",
        "    \n",
        "    print('Epoch {:4d} | loss(train/valid) {:5.2f} / {:5.2f}, ppl(train/valid) {:5.2f} / {:5.2f}, sparsity(current/target) {:.3f} / {:.3f}'.format(\n",
        "        epoch, train_loss, valid_loss,train_ppl, valid_ppl,float(sparsity.cpu()), target_sparsity))\n",
        "    \n",
        "    # tensorboard\n",
        "    writer.add_scalars('joint/loss',{'train_loss': train_loss,'valid_loss': valid_loss},epoch)\n",
        "    writer.add_scalars('joint/perplexity',{'train_ppl': train_ppl,'valid_ppl': valid_ppl},epoch)\n",
        "    writer.add_scalars('joint/sparsity',{'sparsity': float(sparsity.cpu())},epoch)\n",
        "    \n",
        "    print('-'*80)\n",
        "    \n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo2wd-HGbgtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt = model.state_dict()\n",
        "torch.save(ckpt, HOME_DIR + 'model_ntm_s2s_joint.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTprhlxqaLsp",
        "colab_type": "text"
      },
      "source": [
        "## Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqI9FpuRYbH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt = torch.load(HOME_DIR + 'model_ntm_s2s_joint.pt')\n",
        "model.load_state_dict(ckpt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9H6BY3RYjel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ids_to_sentence(vocab, ids):\n",
        "    return [vocab.id2word[_id] for _id in ids]\n",
        "\n",
        "def trim_eos(ids):\n",
        "    if EOS in ids:\n",
        "        return ids[:ids.index(EOS)]\n",
        "    else:\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qE8-TzWqhQr",
        "colab_type": "text"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYWU49iFqgat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataloader = DataLoader(test_X, test_B, test_Y, bow_vocab, batch_size=256,shuffle=False)\n",
        "\n",
        "test_loss = 0.\n",
        "test_bce  = 0.\n",
        "test_hyps = []\n",
        "# validation loop\n",
        "for batch in test_dataloader:\n",
        "    batch_X, _, batch_BV, batch_Y, lengths_X = batch\n",
        "    loss, bce, pred = compute_loss(batch_X, batch_BV, batch_Y, lengths_X, model, optimizer, is_train=False, gamma=200, l1_strength = l1_strength)\n",
        "    test_loss += loss\n",
        "    test_bce  += bce\n",
        " \n",
        "# calc ave total loss\n",
        "valid_loss = np.sum(test_loss) / len(test_dataloader.data)\n",
        "\n",
        "# calc ntm weight sparsity\n",
        "sparsity = check_sparsity(model.ntm.fcd1.weight.data)\n",
        "\n",
        "# calc perplexity\n",
        "test_bce = test_bce / test_dataloader.word_count\n",
        "test_ppl = np.exp(test_bce)\n",
        "\n",
        "\n",
        "print('Loss(test) {:5.2f}, PPL(test) {:5.2f}, sparsity(Test) {:.3f}'.format(\n",
        "    test_loss, test_ppl, float(sparsity.cpu()), ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wMn6qkgiknt",
        "colab_type": "text"
      },
      "source": [
        "#### Test prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clQKBMoninQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataloader = DataLoader(test_X, test_B, test_Y, bow_vocab, batch_size=1,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8UrZO3JYr1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "\n",
        "batch_X, batch_B, batch_BV , batch_Y, lengths_X = next(test_dataloader)\n",
        "sentence_X = ' '.join(ids_to_sentence(vocab_X, batch_X.data.cpu().numpy()[:-1, 0]))\n",
        "sentence_Y = ' '.join(ids_to_sentence(vocab_Y, batch_Y.data.cpu().numpy()[:-1, 0]))\n",
        "\n",
        "print('src: {}'.format(sentence_X))\n",
        "print('tgt: {}'.format(sentence_Y))\n",
        "\n",
        "output, z, g, recon_batch, mu, logvar  = model(batch_X, batch_BV, lengths_X, 5, '')\n",
        "output = output.max(dim=-1)[1].view(-1).data.cpu().tolist()\n",
        "output_sentence = ' '.join(set(ids_to_sentence(vocab_Y, trim_eos(output))))\n",
        "print('out: {}'.format(output_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}